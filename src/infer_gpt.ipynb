{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffa4bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "def load_session(onnx_path: str):\n",
    "    # Enable all available optimizations in ORT\n",
    "    sess_opts = ort.SessionOptions()\n",
    "    so_path = '/home/mcw/work/ONNX_DEV/ops/build/libcustom_op.so'\n",
    "    sess_opts.register_custom_ops_library(so_path)\n",
    "    sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    return ort.InferenceSession(onnx_path, sess_opts)\n",
    "\n",
    "def generate_with_onnx(session, tokenizer, prompt: str, max_length: int = 50):\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = inputs[\"input_ids\"].astype(np.int64)\n",
    "\n",
    "    # Prepare feeds\n",
    "    feeds = {\"input_ids\": input_ids}\n",
    "\n",
    "    # Run the ONNX model\n",
    "    outputs = session.run(None, feeds)\n",
    "    logits = outputs[0]  # shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "    # Greedy decoding: take argmax at each step\n",
    "    generated = input_ids.tolist()[0]\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        next_token = int(np.argmax(last_token_logits))\n",
    "        generated.append(next_token)\n",
    "\n",
    "        # Update input for next iteration\n",
    "        input_ids = np.array([generated], dtype=np.int64)\n",
    "        feeds = {\"input_ids\": input_ids}\n",
    "        outputs = session.run(None, feeds)\n",
    "        logits = outputs[0]\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    onnx_model = \"/home/mcw/work/ONNX_DEV/src/models/onnx_models/gpt2_124M_custom.onnx\"\n",
    "    prompt = \"Once upon a time\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    session = load_session(onnx_model)\n",
    "\n",
    "    result = generate_with_onnx(session, tokenizer, prompt, max_length=100)\n",
    "    print(\"Generated:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090087df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Hello there, I'm sorry. I'm sorry\n",
      "Exiting...\n",
      "Generated: What's wrong for you, why are you saying sorry?\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = input(\"Enter prompt (or 'exit' to quit): \")\n",
    "    result = generate_with_onnx(session, tokenizer, prompt, max_length=10)\n",
    "    print(\"Generated:\", result)\n",
    "\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "    print(\"Exiting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5065ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ort_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
